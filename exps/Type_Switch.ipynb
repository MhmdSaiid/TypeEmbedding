{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "93LGLgBzs7pV",
        "outputId": "96c082d9-6cff-477b-e716-72e0f878143e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 33.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 11.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.0 tokenizers-0.13.2 transformers-4.24.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install SPARQLWRAPPER"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "from transformers import BertTokenizer,BertForMaskedLM\n",
        "from torch.utils.data import TensorDataset,DataLoader,SequentialSampler"
      ],
      "metadata": {
        "id": "alrSK8h_s_QS"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vocab_sim(type_embedding,WE_module,vocab):\n",
        "    '''similairty with PLM vocabulary '''\n",
        "    WE_matrix = WE_module.weight.detach()\n",
        "    sim_scores = torch.nn.functional.cosine_similarity(WE_matrix,type_embedding.reshape(1,-1)).cpu().numpy().tolist()\n",
        "    d = dict(zip(vocab,list(sim_scores)))\n",
        "    sorted_scores = sorted(d.items(),key=lambda x: -x[1])\n",
        "    return sorted_scores\n",
        "\n",
        "def load_jsonl(file):\n",
        "    data=[]\n",
        "    with open(file,'r') as f:\n",
        "        for line in f.readlines():\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "\n",
        "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
        "\n",
        "def get_PoB(subj_id):\n",
        "  query = \"\"\"\n",
        "  SELECT ?obj ?objLabel WHERE\n",
        "  {\n",
        "      wd:!!SUBJ!! wdt:P19 ?obj .\n",
        "      SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
        "  }\n",
        "  \"\"\"\n",
        "  query=query.replace('!!SUBJ!!',subj_id)\n",
        "\n",
        "  sparql.setQuery(query)\n",
        "  sparql.setReturnFormat(JSON)\n",
        "  results = sparql.query().convert()\n",
        "  try:\n",
        "    return results['results']['bindings'][0]['objLabel']['value']\n",
        "  except:\n",
        "    return None"
      ],
      "metadata": {
        "id": "1fF_JoKDtBwt"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GRE_relations = {\n",
        "    \n",
        "        \"place_of_birth_test.jsonl\":\"[X] was born in [Y] .\",\n",
        "\n",
        "        \"date_of_birth_test.jsonl\":\"[X] (born [Y]).\",\n",
        "\n",
        "        \"place_of_death_test.jsonl\":\"[X] died in [Y] .\"\n",
        "}\n",
        "\n",
        "\n",
        "model_arch = 'bert-base-cased'\n",
        "model = BertForMaskedLM.from_pretrained(model_arch)\n",
        "model.eval()\n",
        "tokenizer = BertTokenizer.from_pretrained(model_arch)\n",
        "vocab = list(tokenizer.get_vocab().keys())\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQRdNB2EtDwW",
        "outputId": "64a8c524-d89a-451b-c57e-19077f309124"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=[]\n",
        "file = 'place_of_birth_test.jsonl'\n",
        "concept_json = load_jsonl(file)\n",
        "for x in tqdm(concept_json):\n",
        "  subj_id = x['sub_w']\n",
        "  subj = x['sub_label']\n",
        "  if subj_id:\n",
        "    obj_label = get_PoB(subj_id)\n",
        "    if obj_label:\n",
        "      df.append([subj,obj_label])\n",
        "\n",
        "final_df = pd.DataFrame(df,columns=['SUBJ','OBJ']).drop_duplicates()\n",
        "final_df['sent']=final_df.apply(lambda x:GRE_relations['date_of_birth_test.jsonl'].replace('[X]',x.SUBJ).replace('[Y]','[MASK]'),axis=1)\n",
        "final_df.columns=['SUBJ','gold','sent']\n",
        "final_df=final_df[final_df.gold.apply(lambda x: True if x in vocab else False)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "02XkFatfxW9r",
        "outputId": "d4ccb049-739f-4bd4-ef44-68a91db79416"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2937/2937 [01:19<00:00, 37.14it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.to_csv('DoB_prompts.csv',index=False)"
      ],
      "metadata": {
        "id": "d8BILzFr9_O9"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python MLM_Script.py --file 'DoB_prompts.csv'\\\n",
        "                      --model_arch 'bert-base-cased'\\\n",
        "                      --concept_vector 'TypeVectors/City_vectors.pkl'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHbBg1P6uooQ",
        "outputId": "99b94d34-b178-413c-aa55-7269bc12f208"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r  0% 0/50 [00:00<?, ?it/s]\r100% 50/50 [00:00<00:00, 994.62it/s]\n",
            "\n",
            "\n",
            "\n",
            "k=0\n",
            "100% 8/8 [00:06<00:00,  1.26it/s]\n",
            "P@1:0.0\n",
            "P@10:0.0\n",
            "P@50:0.0\n",
            "P@100:0.0\n",
            "Processing Time:0.10822057723999023\n",
            "Infer Time:6.346968412399292\n",
            "\n",
            "\n",
            "\n",
            "k=1\n",
            "100% 8/8 [00:03<00:00,  2.37it/s]\n",
            "P@1:0.0\n",
            "P@10:0.0\n",
            "P@50:0.0\n",
            "P@100:0.0\n",
            "Processing Time:0.10822057723999023\n",
            "Infer Time:3.379904270172119\n",
            "\n",
            "\n",
            "\n",
            "k=2\n",
            "100% 8/8 [00:03<00:00,  2.06it/s]\n",
            "P@1:0.16\n",
            "P@10:0.28\n",
            "P@50:0.4\n",
            "P@100:0.52\n",
            "Processing Time:0.10822057723999023\n",
            "Infer Time:3.8791086673736572\n",
            "\n",
            "\n",
            "\n",
            "k=3\n",
            "100% 8/8 [00:04<00:00,  1.81it/s]\n",
            "P@1:0.2\n",
            "P@10:0.38\n",
            "P@50:0.54\n",
            "P@100:0.6\n",
            "Processing Time:0.10822057723999023\n",
            "Infer Time:4.423843860626221\n",
            "\n",
            "\n",
            "\n",
            "k=4\n",
            "100% 8/8 [00:05<00:00,  1.58it/s]\n",
            "P@1:0.22\n",
            "P@10:0.38\n",
            "P@50:0.54\n",
            "P@100:0.72\n",
            "Processing Time:0.10822057723999023\n",
            "Infer Time:5.067090749740601\n",
            "\n",
            "\n",
            "\n",
            "k=5\n",
            "100% 8/8 [00:04<00:00,  1.64it/s]\n",
            "P@1:0.22\n",
            "P@10:0.44\n",
            "P@50:0.56\n",
            "P@100:0.74\n",
            "Processing Time:0.10822057723999023\n",
            "Infer Time:4.890537261962891\n",
            "\n",
            "\n",
            "Optimal k: 5\n",
            "100% 939/939 [00:01<00:00, 871.87it/s]\n",
            "100% 12/12 [01:04<00:00,  5.39s/it]\n",
            "P@1:0.14909478168264112\n",
            "P@10:0.4057507987220447\n",
            "P@50:0.6144834930777423\n",
            "P@100:0.7007454739084132\n",
            "Processing Time:1.4023466110229492\n",
            "Infer Time:64.6853985786438\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open('TypeVectors/City_vectors.pkl','rb') as f:\n",
        "  city = pickle.load(f)\n",
        "\n",
        "with open('TypeVectors/Year_vectors.pkl','rb') as f:\n",
        "  year = pickle.load(f)\n",
        "\n",
        "v1 = city['svd_vec']\n",
        "v2 = year['svd_vec']\n",
        "common = torch.dot(v1,v2)/(torch.norm(v1)*torch.norm(v2))\n",
        "city['svd_vec']-=common*year['svd_vec']\n",
        "\n",
        "with open('optim_City_vectors.pkl','wb') as f:\n",
        "  pickle.dump(city,f)"
      ],
      "metadata": {
        "id": "55dXZKqTCWiN"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python MLM_Script.py --file 'DoB_prompts.csv'\\\n",
        "                      --model_arch 'bert-base-cased'\\\n",
        "                      --concept_vector 'optim_City_vectors.pkl'\\\n",
        "                      --manual\\\n",
        "                      --k 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryouW5IeCgI5",
        "outputId": "f9448a0d-649f-45ea-d368-0587b4f73848"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 939/939 [00:00<00:00, 1773.66it/s]\n",
            "100% 12/12 [00:50<00:00,  4.22s/it]\n",
            "P@1:0.18743343982960597\n",
            "P@10:0.4440894568690096\n",
            "P@50:0.617678381256656\n",
            "P@100:0.7145899893503728\n",
            "Processing Time:0.7427895069122314\n",
            "Infer Time:50.659167766571045\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df_PoB.to_csv('PoB_prompts.csv',index=False)"
      ],
      "metadata": {
        "id": "C5ewt_NcDhu6"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python MLM_Script.py --file 'PoB_prompts.csv'\\\n",
        "                      --model_arch 'bert-base-cased'\\\n",
        "                      --concept_vector 'City_vectors.pkl'\\\n",
        "                      --manual\\\n",
        "                      --k 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGuoRPHYDVhK",
        "outputId": "c30768ab-b654-4426-c9d2-a5c09d670c6d"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 939/939 [00:00<00:00, 1658.50it/s]\n",
            "100% 12/12 [00:50<00:00,  4.21s/it]\n",
            "P@1:0.24387646432374868\n",
            "P@10:0.5282215122470714\n",
            "P@50:0.7273695420660277\n",
            "P@100:0.8072417465388712\n",
            "Processing Time:0.7663583755493164\n",
            "Infer Time:50.54681444168091\n",
            "\n"
          ]
        }
      ]
    }
  ]
}